{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from nltk import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thota\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thota\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2905: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pose sequence as a NLI premise and label as a hypothesis\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "premise = sequence_to_classify\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "hypothesis = f'This example is {candidate_labels[0]}.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                     truncation_strategy='only_first')\n",
    "logits = nli_model(x)[0]\n",
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0055, 0.9945]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    theme_classifier = pipeline(\n",
    "        'zero-shot-classification',\n",
    "        model='facebook/bart-large-mnli'\n",
    "    )\n",
    "    return theme_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     TFSeq2SeqSequenceClassifierOutput,\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzero-shot-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-mnli\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    896\u001b[0m         model,\n\u001b[0;32m    897\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    898\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    899\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    900\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    901\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    902\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    903\u001b[0m     )\n\u001b[0;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:261\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 261\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1594\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'one day I will see the world',\n",
       " 'labels': ['travel', 'dancing', 'cooking'],\n",
       " 'scores': [0.9938651919364929, 0.0032738028094172478, 0.002861030399799347]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "pipe(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/Subtitles\\\\Naruto Season 1 - 01.ass',\n",
       " 'data/Subtitles\\\\Naruto Season 1 - 02.ass',\n",
       " 'data/Subtitles\\\\Naruto Season 1 - 03.ass',\n",
       " 'data/Subtitles\\\\Naruto Season 1 - 04.ass',\n",
       " 'data/Subtitles\\\\Naruto Season 1 - 05.ass']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "from glob import glob\n",
    "files = glob('data/Subtitles/*.ass')\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (files[0], 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = lines[27:]\n",
    "    lines = [','.join(line.split(',')[9:]) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A long time ago, a powerful demon fox\\\\Nappeared with nine tails.\\n',\n",
       " 'With its powerful tails,\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(files[0].split('-')[-1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subtitles_dataset(dataset_path):\n",
    "    subtitles_path = glob('data/Subtitles/*.ass')\n",
    "    scripts = []\n",
    "    episode_num = []\n",
    "    for path in subtitles_path:\n",
    "        # Read Lines\n",
    "        with open (path, 'r', encoding=\"utf8\") as f:\n",
    "            lines = f.readlines()\n",
    "            lines = lines[27:]\n",
    "            lines = [','.join(line.split(',')[9:]) for line in lines]\n",
    "        lines = [line.replace('\\\\N',' ') for line in lines]\n",
    "        script = \" \".join(lines)\n",
    "\n",
    "        episode = int(path.split('-')[-1].split('.')[0].strip())\n",
    "        scripts.append(script)\n",
    "        episode_num.append(episode)\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"episode\":episode_num, \"script\": scripts})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script\n",
       "0        1  A long time ago, a powerful demon fox appeared...\n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "2        3  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "3        4  C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "4        5  C'mon!\\n Running like a fugitive,\\n Being chas..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'data/Subtitles/'\n",
    "df = load_subtitles_dataset(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['episode'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A long time ago, a powerful demon fox appeared...\n",
       "1    C'mon!\\n Running like a fugitive,\\n Being chas...\n",
       "Name: script, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\2711167983.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  script[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A long time ago, a powerful demon fox appeared with nine tails.\\n With its powerful tails,\\n it could smash mountains and create tidal waves.\\n A band of Ninjas rose to defend their village from attack.\\n We have to wait until the Fourth Hokage gets here!\\n We can\\'t let it get any closer to our village!\\n One great Ninja was able to imprison the monster,\\n but died in the process.\\n This Ninja was known as… the Fourth Hokage.\\n Naruto!\\n Why did you do such a thing?!\\n You\\'re really gonna get it this time!\\n I don\\'t care!\\n You know your problem?\\n You can\\'t do the things I do!\\n Only I can do this!\\n I\\'m better than all of you! Believe it!\\n There\\'s a problem, sir!\\n Lord Hokage!\\n What is it?\\n Did that Naruto do something again?\\n Yes. He climbed onto the Mountainside Images…\\n And he vandalized and graffitied all over them!\\n Wait!\\n Ha ha…\\n Why should I?\\n Hey, Naruto!\\n How did you suddenly get here, lruka Sensei?\\n The question is what are you doing here when you should be in class now?\\n Now listen, Naruto.\\n You failed the last graduation test and the one before that.\\n This is no time to be goofing off, you fool!\\n We will have a re-test on the Transformation Jutsu!\\n Even those who already passed will take it!\\n Whaaaat?!\\n Sakura Haruno. Here I go…\\n Transform!\\n OK!\\n I did it!\\n Cha!\\n Did you see that, Sasuke?\\n Next, Sasuke Uchiha.\\n Yes.\\n O-OK.\\n Next, Naruto Uzumaki.\\n This is a real pain.\\n And it\\'s all your fault.\\n Like I care!!\\n OK…\\n Good luck, Naruto…\\n Transform!\\n How was it?\\n I call it the \"Sexy Jutsu\"!\\n You fool! Stop making idiotic spells!\\n Darn…\\n Darn…\\n I won\\'t let you go home unless you clean that all up.\\n I don\\'t care…\\n There\\'s nobody home anyway.\\n Naruto...\\n What is it this time?\\n What I meant was…\\n If you clean up all that mess, I\\'ll buy you ramen tonight.\\n Huh?!\\n Yes! I-I will finish it no time!\\n Enter: Naruto Uzumaki!\\n Naruto.\\n Why did you vandalize those faces?\\n Don\\'t you know who the Hokage leaders are?\\n Of course, I do!\\n I know they earned the title Lord Hokage\\n because they were the best Ninja of their time, right?\\n Especially the Fourth Hokage was a hero\\n who saved the village from the nine-tail demon fox.\\n Then why did you do that?\\n Because I\\'ll become a Hokage myself.\\n And I\\'ll be the greatest Hokage of all time!\\n So that everyone will finally learn to accept me!\\n By the way, Sensei, I have a favor to ask.\\n You want another bowl?\\n Mmmm…No…\\n Can I borrow that Leaf headband for a while?\\n This?\\n No no!\\n This is worn only by those who have graduated from Ninja Academy.\\n Tomorrow, you will…\\n You\\'re so mean!\\n So that\\'s why you took off your goggles…\\n Humph... One more bowl please!\\n We are now about to begin the graduation test.\\n When your name is called, proceed to the next classroom.\\n The test is on the Clone Jutsu.\\n Oh no…\\n Of all the…! That is my weakest Jutsu!\\n But still… I will do it no matter what!\\n Clone Jutsu!\\n Disqualified!\\n Iruka Sensei.\\n His physical coordination and stamina are excellent.\\n And he managed to come up with something.\\n Isn\\'t that enough for him to pass?\\n Mizuki Sensei... All the others created three or more clones.\\n Naruto created just one.\\n And it\\'s practically useless. I can\\'t give him a passing mark.\\n I \\'m a Ninja now!\\n You did well. That\\'s my son.\\n Congratulations for your graduation.\\n I\\'ll cook something good tonight!\\n Look at that one.\\n It\\'s that boy. I hear he\\'s the only one who failed.\\n Serves him right.\\n Imagine what would happen if he became a Ninja.\\n Isn\\'t that the boy who is actually…\\n Hey! We\\'re not supposed to talk about that.\\n Iruka. We need to talk later.\\n Yes, sir.\\n Iruka Sensei isn\\'t trying to be mean to you.\\n Then why only me?\\n He wants you to become strong from the bottom of his heart.\\n You both don\\'t have parents.\\n But I really wanted to graduate.\\n Heh... I guess I have no choice…\\n I\\'ll let you in on a big secret.\\n Secret?\\n Iruka.\\n What is it, Lord Hokage?\\n I know how you feel. But…\\n Naruto also grew up without knowing the love of his parents…like you.\\n Let me go!\\n My mom and dad are still out there fighting!\\n Wake up, Iruka Sensei!\\n What\\'s the matter?\\n Come to Lord Hokage\\'s immediately!\\n I heard that Naruto… stole the Scroll of Sealing.\\n The Scroll of Sealing?!\\n Let\\'s see…\\n The first Jutsu is… Multi-Shadow Clone Jutsu?\\n What?! Already a Jutsu I\\'m no good at?\\n Lord Hokage! We can\\'t forgive him!\\n This is not just a prank!\\n The Scroll is a dangerous item that the First Hokage sealed!\\n Depending on its use…\\n It will be a major disaster if it is taken out of the village!\\n Yes. Bring Naruto here at once!\\n Yes, sir!\\n Where did you go…Naruto?\\n I will tell everyone in the village about this and eliminate Naruto…\\n Then the Scroll of Sealing will be mine!\\n Hey you, Naruto!\\n You found me..\\n And I\\'ve only learned one Jutsu.\\n He\\'s been practicing the Jutsu…\\n until he\\'s become this exhausted and dirty…?\\n Listen, listen! I\\'m gonna show you this amazing Jutsu!\\n You\\'re gonna let me graduate if I can do it!\\n Isn\\'t it true that I can graduate if I can do one of the Jutsu written here?\\n Who told you that?\\n Mizuki Sensei. He told me about this scroll, and this place…\\n Mizuki did?!\\n I\\'m impressed you found this place.\\n I see now…how it is.\\n Naruto, give me that scroll.\\n Wait, wait… What\\'s going on here?\\n Naruto! Never give him that scroll!\\n It is a dangerous object that contains forbidden Ninja Jutsu. It was sealed.\\n Mizuki used you in order to get it for himself!\\n W-Wha--?\\n Naruto, Iruka is only afraid of you holding that scroll!\\n Huh?\\n What are you saying, Mizuki! Don\\'t let him fool you, Naruto!\\n I will tell you the truth.\\n Idiot! Don\\'t do that!\\n After an incident 12 years ago, a rule was created.\\n A rule?\\n That is, Naruto, a rule everybody but you knows.\\n Except me?! \\t\\t\\t\\t\\tWhat is it?\\n Stop it, Mizuki!\\n The rule forbids anyone from revealing that you are actually the Demon Fox Spirit!\\n Huh?\\n You are actually the Demon, Nine-Tailed Fox Spirit,\\n who killed Iruka\\'s parents and destroyed our village!\\n Stop it!\\n Everyone has been deceiving you ever since.\\n Didn\\'t you find it strange?\\n Why everyone hated you so much?\\n No! No! No! No! No!\\n Naruto…\\n Nobody accepts you. That\\'s why Iruka hates you so much!\\n Iruka... Naruto grew up without the love of parents.\\n Everyone avoids him like the plague after what happened.\\n That\\'s why he keeps misbehaving.\\n It\\'s the only way for him to get any attention or acknowledgement.\\n He pretends to be tough, but inside he is really hurting.\\n Die, Naruto!\\n Naruto! \\t\\t\\t\\t\\tGet down!\\n Why…?\\n Because you and I are the same.\\n After my parents died,\\n nobody paid attention to me or gave me any support.\\n I wasn\\'t a good student in school.\\n I was the class clown… because I wanted people to notice me.\\n I couldn\\'t get noticed through excellence, so I kept doing stupid things.\\n It was so hard.\\n Isn\\'t that right, Naruto?\\n You felt so lonely…right? And you suffered inside, right?\\n I\\'m sorry, Naruto….\\n If I had been more responsible, maybe you wouldn\\'t have suffered so much.\\n Don\\'t make me laugh!\\n Iruka has always hated you, ever since you killed his parents!\\n He\\'s just saying all that to get the Scroll of Sealing back!\\n Naruto!\\n Narutoooooo!\\n He is not the type of kid who will change his mind.\\n He will take revenge against our village using that scroll!\\n Didn\\'t you see his eyes? Those are the eyes of a Demon Fox.\\n No… Naruto…isn\\'t…like that at all!\\n All I want is to kill Naruto and get the scroll.\\n I\\'ll take care of you later!\\n I-I won\\'t let you…\\n Well, well.\\n Mizuki has a big mouth!\\n Naruto feels worse than he\\'s ever felt.\\n He might unleash the power locked up inside him.\\n The Scroll of Sealing is now with him.\\n There\\'s a slight chance he might actually release\\n the Nine-Tailed Fox Spirit sealed inside him!\\n If that happens…\\n I\\'ve found him!\\n Naruto!\\n Everything that Mizuki said was a lie!\\n Give me that scroll, quick! Mizuki is after the scroll!\\n It can\\'t be…\\n Why is it, Naruto?\\n How…\\n did you know I wasn\\'t Iruka…?\\n Because I\\'m Iruka.\\n I see.\\n What\\'s in it for you to protect the one who killed your family?\\n I\\'m not gonna let a stupid idiot like you get that scroll!\\n You\\'re the idiot. Naruto is the same as me.\\n Same?\\n Anyone can do whatever he wants once he has the scroll.\\n There is no way that that monster…\\n that Fox Spirit, won\\'t take advantage of the power of that scroll!\\n You\\'re right…\\n I guess it was true all along!\\n See, Iruka Sensei never really cared for me at all!\\n ...if he was the Demon Fox Spirit.\\n But Naruto is different!\\n I know that he is an exceptional student.\\n He works very hard,\\n and he\\'s single-minded and clumsy at the same time.\\n No one accepts him, but he knows the meaning of human suffering.\\n He is not the Demon Fox Spirit.\\n He\\'s Naruto Uzumaki of the Village Hidden in the Leaves!\\n You are so gullible. \\t\\t\\t\\t\\tlruka!\\n I was gonna take you down later, but I have changed my mind.\\n Die!\\n I guess this is the end for me…\\n Naruto?!\\n You surprised me there, freak.\\n If you ever lay a hand on Iruka Sensei, I\\'ll kill you!\\n Shut up! I can take care of a kid like you with a single blow!\\n Why don\\'t you try then? I\\'ll strike you back a thousand-fold!\\n Let\\'s see you try! Show me what you can do, Demon Fox!\\n Shadow Clone Jutsu!\\n Naruto! You\\'ve…\\n Those aren\\'t just images but actual clones! That\\'s an advanced Ninjutsu!\\n What\\'s this…?\\n What\\'s the matter? \\t\\t\\t\\t\\tC\\'mon!\\n Weren\\'t you gonna get me with one blow? Here!\\n In that case… I\\'ll come to you.\\n I kinda got carried away. lruka Sensei, are you okay?\\n Yeah.\\n He\\'s really something.\\n Maybe it is true.\\n Maybe he will surpass all the Hokage leaders…\\n Naruto, come over here. I\\'d like to give you something.\\n Has anyone found Naruto yet?\\n No.\\n Darn, this is going to be bad…\\n There\\'s no need to worry anymore.\\n Lord Hokage!\\n He\\'ll be back soon.\\n Sensei, how much longer?\\n OK, you may open your eyes now.\\n Congratulations…on your graduation.\\n In celebration, we\\'ll have ramen tonight!\\n Iruka Sensei!\\n That hurts!\\n Naruto…\\n I was going to lecture to you...\\n that the road gets more difficult now that you\\'re a Ninja.\\n But I guess I\\'ll just wait to tell you that until we get to the ramen stand…\\n W-What do you want, you little shrimp? Quit following me!\\n You\\'re smaller than me and\\n you\\'re saying that you\\'re gonna become the Fifth Hokage?\\n I don\\'t care if you are the 3rd Hokage\\'s grandson or not.\\n It\\'s not that easy to be a Hokage!\\n If you want it that bad, you\\'re gonna have to beat me first!\\n Next episode:  \"My Name Is Konohamaru!\"\\n Watch my outstanding performance!\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run Model\n",
    "script = df.iloc[0]\n",
    "script[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A long time ago, a powerful demon fox appeared with nine tails.',\n",
       " 'With its powerful tails,\\n it could smash mountains and create tidal waves.',\n",
       " 'A band of Ninjas rose to defend their village from attack.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "script_sentences = sent_tokenize(script[1])\n",
    "script_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Sentence\n",
    "sentence_batch_size = 20\n",
    "script_btaches=[]\n",
    "for index in range(0, len(script_sentences), sentence_batch_size):\n",
    "    sent = \" \".join(script_sentences[index:index+sentence_batch_size])\n",
    "    script_btaches.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A long time ago, a powerful demon fox appeared with nine tails. With its powerful tails,\\n it could smash mountains and create tidal waves. A band of Ninjas rose to defend their village from attack. We have to wait until the Fourth Hokage gets here! We can't let it get any closer to our village! One great Ninja was able to imprison the monster,\\n but died in the process. This Ninja was known as… the Fourth Hokage. Naruto! Why did you do such a thing?! You're really gonna get it this time! I don't care! You know your problem? You can't do the things I do! Only I can do this! I'm better than all of you! Believe it! There's a problem, sir! Lord Hokage! What is it? Did that Naruto do something again?\",\n",
       " 'Yes. He climbed onto the Mountainside Images…\\n And he vandalized and graffitied all over them! Wait! Ha ha…\\n Why should I? Hey, Naruto! How did you suddenly get here, lruka Sensei? The question is what are you doing here when you should be in class now? Now listen, Naruto. You failed the last graduation test and the one before that. This is no time to be goofing off, you fool! We will have a re-test on the Transformation Jutsu! Even those who already passed will take it! Whaaaat?! Sakura Haruno. Here I go…\\n Transform! OK! I did it! Cha! Did you see that, Sasuke? Next, Sasuke Uchiha.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_btaches[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_list=['battle','love','friendhip','hate','happy','sacrifice','self development','betrayal','dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 4 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 5 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 6 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 7 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 8 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 9 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 10 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 11 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 12 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 13 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 14 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 15 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 16 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D51B130> was reported to be 2(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "pipe_out = pipe(\n",
    "    script_btaches[:2],\n",
    "    theme_list,\n",
    "    multi_label=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"A long time ago, a powerful demon fox appeared with nine tails. With its powerful tails,\\n it could smash mountains and create tidal waves. A band of Ninjas rose to defend their village from attack. We have to wait until the Fourth Hokage gets here! We can't let it get any closer to our village! One great Ninja was able to imprison the monster,\\n but died in the process. This Ninja was known as… the Fourth Hokage. Naruto! Why did you do such a thing?! You're really gonna get it this time! I don't care! You know your problem? You can't do the things I do! Only I can do this! I'm better than all of you! Believe it! There's a problem, sir! Lord Hokage! What is it? Did that Naruto do something again?\",\n",
       "  'labels': ['dialogue',\n",
       "   'betrayal',\n",
       "   'battle',\n",
       "   'sacrifice',\n",
       "   'self development',\n",
       "   'friendhip',\n",
       "   'hate',\n",
       "   'happy',\n",
       "   'love'],\n",
       "  'scores': [0.9800741672515869,\n",
       "   0.939690351486206,\n",
       "   0.8546885251998901,\n",
       "   0.7349815964698792,\n",
       "   0.728497326374054,\n",
       "   0.567444384098053,\n",
       "   0.2801382541656494,\n",
       "   0.06682592630386353,\n",
       "   0.040261901915073395]},\n",
       " {'sequence': 'Yes. He climbed onto the Mountainside Images…\\n And he vandalized and graffitied all over them! Wait! Ha ha…\\n Why should I? Hey, Naruto! How did you suddenly get here, lruka Sensei? The question is what are you doing here when you should be in class now? Now listen, Naruto. You failed the last graduation test and the one before that. This is no time to be goofing off, you fool! We will have a re-test on the Transformation Jutsu! Even those who already passed will take it! Whaaaat?! Sakura Haruno. Here I go…\\n Transform! OK! I did it! Cha! Did you see that, Sasuke? Next, Sasuke Uchiha.',\n",
       "  'labels': ['dialogue',\n",
       "   'self development',\n",
       "   'battle',\n",
       "   'betrayal',\n",
       "   'sacrifice',\n",
       "   'friendhip',\n",
       "   'hate',\n",
       "   'happy',\n",
       "   'love'],\n",
       "  'scores': [0.9370126724243164,\n",
       "   0.8678210973739624,\n",
       "   0.6581321954727173,\n",
       "   0.6457260847091675,\n",
       "   0.6258837580680847,\n",
       "   0.42257246375083923,\n",
       "   0.1983579695224762,\n",
       "   0.08490363508462906,\n",
       "   0.02802049182355404]}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes= {}\n",
    "for output in pipe_out:\n",
    "    for label,score in zip(output['labels'], output['scores']):\n",
    "        if label not in themes:\n",
    "            themes[label] = []\n",
    "        themes[label].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': [0.9800741672515869, 0.9370126724243164],\n",
       " 'betrayal': [0.939690351486206, 0.6457260847091675],\n",
       " 'battle': [0.8546885251998901, 0.6581321954727173],\n",
       " 'sacrifice': [0.7349815964698792, 0.6258837580680847],\n",
       " 'self development': [0.728497326374054, 0.8678210973739624],\n",
       " 'friendhip': [0.567444384098053, 0.42257246375083923],\n",
       " 'hate': [0.2801382541656494, 0.1983579695224762],\n",
       " 'happy': [0.06682592630386353, 0.08490363508462906],\n",
       " 'love': [0.040261901915073395, 0.02802049182355404]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_themes_inference(script):\n",
    "    script_sentences = sent_tokenize(script)\n",
    "\n",
    "    #Batch Sentence\n",
    "    sentence_batch_size = 20\n",
    "    script_btaches=[]\n",
    "    for index in range(0, len(script_sentences), sentence_batch_size):\n",
    "        sent = \" \".join(script_sentences[index:index+sentence_batch_size])\n",
    "        script_btaches.append(sent)\n",
    "    # Run Model\n",
    "    theme_out = pipe(\n",
    "        script_btaches[:2],\n",
    "        theme_list,\n",
    "        multi_label=True\n",
    "    )\n",
    "    # Wrangle output\n",
    "    themes= {}\n",
    "    for output in pipe_out:\n",
    "        for label,score in zip(output['labels'], output['scores']):\n",
    "            if label not in themes:\n",
    "                themes[label] = []\n",
    "            themes[label].append(score)   \n",
    "    themes = {key: np.mean(np.array(val)) for key,val in themes.items()}\n",
    "    return themes          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = {key: np.mean(np.array(val)) for key,val in themes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': 0.9585434198379517,\n",
       " 'betrayal': 0.7927082180976868,\n",
       " 'battle': 0.7564103603363037,\n",
       " 'sacrifice': 0.6804326772689819,\n",
       " 'self development': 0.7981592118740082,\n",
       " 'friendhip': 0.4950084239244461,\n",
       " 'hate': 0.2392481118440628,\n",
       " 'happy': 0.07586478069424629,\n",
       " 'love': 0.03414119686931372}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script\n",
       "0        1  A long time ago, a powerful demon fox appeared...\n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 4 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 5 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 6 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 7 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 8 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 9 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 10 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 11 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 12 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 13 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 14 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 15 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 16 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D477610> was reported to be 2(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 4 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 5 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 6 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 7 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 8 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 9 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 10 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 11 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 12 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 13 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 14 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 15 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 16 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 17 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "d:\\DataScience\\DeepLearning\\NLP_TV_Series_Project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:641: UserWarning: Length of IterableDataset <transformers.pipelines.pt_utils.PipelineChunkIterator object at 0x000001499D206470> was reported to be 2(when accessing len(dataloader)), but 18 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "output_themes = df['script'].apply(get_themes_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'dialogue': 0.9585434198379517, 'betrayal': 0...\n",
       "1    {'dialogue': 0.9585434198379517, 'betrayal': 0...\n",
       "Name: script, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_df = pd.DataFrame(output_themes.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>battle</th>\n",
       "      <th>sacrifice</th>\n",
       "      <th>self development</th>\n",
       "      <th>friendhip</th>\n",
       "      <th>hate</th>\n",
       "      <th>happy</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.792708</td>\n",
       "      <td>0.75641</td>\n",
       "      <td>0.680433</td>\n",
       "      <td>0.798159</td>\n",
       "      <td>0.495008</td>\n",
       "      <td>0.239248</td>\n",
       "      <td>0.075865</td>\n",
       "      <td>0.034141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.792708</td>\n",
       "      <td>0.75641</td>\n",
       "      <td>0.680433</td>\n",
       "      <td>0.798159</td>\n",
       "      <td>0.495008</td>\n",
       "      <td>0.239248</td>\n",
       "      <td>0.075865</td>\n",
       "      <td>0.034141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialogue  betrayal   battle  sacrifice  self development  friendhip  \\\n",
       "0  0.958543  0.792708  0.75641   0.680433          0.798159   0.495008   \n",
       "1  0.958543  0.792708  0.75641   0.680433          0.798159   0.495008   \n",
       "\n",
       "       hate     happy      love  \n",
       "0  0.239248  0.075865  0.034141  \n",
       "1  0.239248  0.075865  0.034141  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n",
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n",
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n",
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n",
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n",
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n",
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n",
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n",
      "C:\\Users\\thota\\AppData\\Local\\Temp\\ipykernel_14572\\3417989627.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[theme_df.columns] = theme_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>battle</th>\n",
       "      <th>sacrifice</th>\n",
       "      <th>self development</th>\n",
       "      <th>friendhip</th>\n",
       "      <th>hate</th>\n",
       "      <th>happy</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.792708</td>\n",
       "      <td>0.75641</td>\n",
       "      <td>0.680433</td>\n",
       "      <td>0.798159</td>\n",
       "      <td>0.495008</td>\n",
       "      <td>0.239248</td>\n",
       "      <td>0.075865</td>\n",
       "      <td>0.034141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.792708</td>\n",
       "      <td>0.75641</td>\n",
       "      <td>0.680433</td>\n",
       "      <td>0.798159</td>\n",
       "      <td>0.495008</td>\n",
       "      <td>0.239248</td>\n",
       "      <td>0.075865</td>\n",
       "      <td>0.034141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script  dialogue  \\\n",
       "0        1  A long time ago, a powerful demon fox appeared...  0.958543   \n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas...  0.958543   \n",
       "\n",
       "   betrayal   battle  sacrifice  self development  friendhip      hate  \\\n",
       "0  0.792708  0.75641   0.680433          0.798159   0.495008  0.239248   \n",
       "1  0.792708  0.75641   0.680433          0.798159   0.495008  0.239248   \n",
       "\n",
       "      happy      love  \n",
       "0  0.075865  0.034141  \n",
       "1  0.075865  0.034141  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[theme_df.columns] = theme_df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize output\n",
    "\n",
    "df1 = df.drop('dialogue', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>script</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>battle</th>\n",
       "      <th>sacrifice</th>\n",
       "      <th>self development</th>\n",
       "      <th>friendhip</th>\n",
       "      <th>hate</th>\n",
       "      <th>happy</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A long time ago, a powerful demon fox appeared...</td>\n",
       "      <td>0.792708</td>\n",
       "      <td>0.75641</td>\n",
       "      <td>0.680433</td>\n",
       "      <td>0.798159</td>\n",
       "      <td>0.495008</td>\n",
       "      <td>0.239248</td>\n",
       "      <td>0.075865</td>\n",
       "      <td>0.034141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C'mon!\\n Running like a fugitive,\\n Being chas...</td>\n",
       "      <td>0.792708</td>\n",
       "      <td>0.75641</td>\n",
       "      <td>0.680433</td>\n",
       "      <td>0.798159</td>\n",
       "      <td>0.495008</td>\n",
       "      <td>0.239248</td>\n",
       "      <td>0.075865</td>\n",
       "      <td>0.034141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode                                             script  betrayal  \\\n",
       "0        1  A long time ago, a powerful demon fox appeared...  0.792708   \n",
       "1        2  C'mon!\\n Running like a fugitive,\\n Being chas...  0.792708   \n",
       "\n",
       "    battle  sacrifice  self development  friendhip      hate     happy  \\\n",
       "0  0.75641   0.680433          0.798159   0.495008  0.239248  0.075865   \n",
       "1  0.75641   0.680433          0.798159   0.495008  0.239248  0.075865   \n",
       "\n",
       "       love  \n",
       "0  0.034141  \n",
       "1  0.034141  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>betrayal</td>\n",
       "      <td>1.585416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>battle</td>\n",
       "      <td>1.512821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sacrifice</td>\n",
       "      <td>1.360865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>self development</td>\n",
       "      <td>1.596318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>friendhip</td>\n",
       "      <td>0.990017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hate</td>\n",
       "      <td>0.478496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happy</td>\n",
       "      <td>0.151730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>love</td>\n",
       "      <td>0.068282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              theme     score\n",
       "0          betrayal  1.585416\n",
       "1            battle  1.512821\n",
       "2         sacrifice  1.360865\n",
       "3  self development  1.596318\n",
       "4         friendhip  0.990017\n",
       "5              hate  0.478496\n",
       "6             happy  0.151730\n",
       "7              love  0.068282"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_output = df1.drop(['episode','script'], axis=1).sum().reset_index()\n",
    "theme_output.columns=['theme', 'score']\n",
    "theme_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(data=theme_output, x='theme',y='score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
